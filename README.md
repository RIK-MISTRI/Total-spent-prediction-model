Total Spent Prediction Model (Ridge Regression)üåü Project OverviewThis project focuses on building a highly accurate regression model to predict the Total Spent for a retail transaction. The core challenge in the original dataset was the strong multicollinearity between $\text{Price Per Unit}$, $\text{Quantity}$, and the target variable $\text{Total Spent}$ (since $\text{Total Spent} \approx \text{Price} \times \text{Quantity}$).The solution involved a crucial Feature Engineering step to address this multiplicative relationship, leading to an exceptionally high-performing and stable Ridge Regression model.üìà Final Performance SummaryThe model was trained on $70\%$ of the data and evaluated on the remaining $30\%$ (test set).MetricValueAssessment$R^2$ Score0.9516Outstanding. The model explains over 95% of the variance in $\text{Total Spent}$.RMSE$20.64$The average prediction error is approximately $\$20.64$.Error Rate$15.92\%$The RMSE is only $15.92\%$ of the average $\text{Total Spent}$ ($\$129.65$).üõ†Ô∏è Methodology: The Key to SuccessThe high performance was achieved through three essential data science steps:1. Feature Engineering (The Interaction Term)Standard Linear Regression performs poorly on multiplicative relationships. The core problem was solved by creating an Interaction Feature:$$\mathbf{\text{Quantity\_x\_PricePerUnit}} = \text{Quantity} \times \text{Price Per Unit}$$This single feature transforms the non-linear multiplicative relationship into a stable, linear one, as visually confirmed by the scatter plot: .2. Data Preprocessing & EncodingAll features were converted to a machine-readable format:Missing Values: All missing values were imputed using the mean (for numeric columns) or the mode (for categorical columns).Binary Encoding: $\text{Discount Applied}$ (True/False) and $\text{Location}$ (Online/In-store) were converted to 0/1 binary columns using $\text{OneHotEncoder}$ with drop='first'.Ordinal Encoding: $\text{Payment Method}$ and $\text{Category}$ were converted to numerical codes using $\text{OrdinalEncoder}$.Scaling: All features were standardized using $\text{StandardScaler}$ to prevent any feature from dominating the model due to its scale.3. Modeling and RegularizationA Ridge Regression model ($\alpha=0.1$) was chosen because it helps stabilize the coefficients and manage any minor residual multicollinearity that might remain after encoding the categorical variables.üßê Final Model Interpretation (Business Insights)Since the features were scaled, the magnitude of the coefficients indicates the relative importance of each factor in predicting $\text{Total Spent}$. The $\text{Intercept}$ of $\mathbf{\$129.63}$ represents the baseline $\text{Total Spent}$ for an average transaction.FeatureCoefficientRelative ImportanceInterpretation$\text{Quantity\_x\_PricePerUnit}$$90.19$Extremely HighThe primary predictor, validating the feature engineering approach.$\text{Category}$$0.34$Highest Marginal Impact (Positive)Transactions in categories with higher ordinal codes tend to have a slightly higher $\text{Total Spent}$, all else being equal.$\text{Location}$$-0.14$Negative Marginal ImpactTransactions in the $\text{Location}$ encoded as $1$ (likely In-store) tend to have a slightly lower $\text{Total Spent}$ compared to the other location (Online), holding $\text{Price} \times \text{Quantity}$ constant.$\text{Payment Method}$$0.10$Small Marginal Impact (Positive)Transactions with a higher-coded $\text{Payment Method}$ are associated with a minor increase in $\text{Total Spent}$.üíª Repository StructureThe project is structured to allow easy reproduction of the results:retail_store_sales.csv: The raw dataset (omitted for size, if needed).total_spent_prediction.ipynb: The main Jupyter Notebook detailing all steps from data loading, preprocessing, feature engineering, scaling, training, evaluation, and final interpretation.Corelation_heatmap.png: The initial correlation analysis.model_visualization.png: The scatter plot validating the interaction feature.
